---
layout: default
---


# Nonlinear Algebra Seminar
December 12, 1:10 - 5 pm\\
Evans 740, UC Berkeley

## Schedule

| 1:10 - 1:40 | Ruriko Yoshida |
| 1:40 - 2:10 | Marco David |
| 2:10 - 2:40 | Svala Sverrisdóttir  |
| 2:40 - 2:50 | Break |
| 2:50 - 3:20 | Maksym Zubkov |
| 3:20 - 3:50 | Kristen Dawson  |
| 3:50 - 4:00 | Break |
| 4:00 - 5:00 | Dmitriy Morozov |

## Titles and Abstracts




<span class="header-color">Speaker:</span>
[Ruriko Yoshida](http://www.polytopes.net){:target="_blank"} (Naval Postgradaute School)\\
<span class="header-color">Title:</span>
Tropical Fermat-Weber Polytropes\\
<span class="header-color">Abstract:</span>
In this talk we discuss the geometry of tropical Fermat-Weber points in terms of the symmetric tropical metric over the tropical projective torus.  It is well-known that a tropical Fermat-Weber point of a given sample is not unique and we show that the set of all possible Fermat-Weber points forms a polytrope. To prove this, we show that the tropical Fermat-Weber polytrope is a bounded cell of a tropical hyperplane arrangement given by both max- and min-tropical hyperplanes with apices given by the sample. We also define tropical Fermat-Weber gradients and provide a gradient descent algorithm that converges to the Fermat-Weber polytrope.  This is joint work with J. Sabol, D. Barnhill and K. Miura. 


<span class="header-color">Speaker:</span>
[Marco David](https://physics.berkeley.edu/people/marco-david){:target="_blank"} (UC Berkeley)\\
<span class="header-color">Title:</span>
Symplectic Learning for Hamiltonian Neural Networks\\
<span class="header-color">Abstract:</span>
Machine learning methods are widely used to model and predict physical systems from observation data. Yet, they are often used as poorly understood “black boxes,” disregarding existing mathematical structure and invariants of the problem. The proposal of Hamiltonian Neural Networks (HNNs) from 2019 takes a first step towards a “gray box” approach, using physical insight to improve performance. Here, we explore a significantly improved training method for HNNs, exploiting the symplectic structure of Hamiltonian systems. This frees the loss from an artificial lower bound. Moreover, we are able to prove detailed analytic bounds on the training errors of HNNs which, in turn, renders them explainable. Finally, we present a post-training correction to obtain the true Hamiltonian only from discretized observation data, up to an arbitrary order.

<span class="header-color">Speaker:</span>
[Svala Sverissdóttir](https://math.berkeley.edu/~svala/){:target="_blank"} (UC Berkeley)\\
<span class="header-color">Title:</span>
Gram Matrices for Isotropic Vectors\\
<span class="header-color">Abstract:</span>
We investigate determinantal varieties
for symmetric matrices that have
zero blocks along the main diagonal.
In theoretical physics,  these arise as Gram matrices for
kinematic variables in quantum field theories.
We study the ideals of relations among
functions in the matrix entries
that serve as building blocks for conformal correlators.





<span class="header-color">Speaker:</span>
[Maksym Zubkov](https://maksymzubkov.info){:target="_blank"} (University of British Columbia)\\
<span class="header-color">Title:</span>
The Geometry of Rational Neural Networks \\
<span class="header-color">Abstract:</span>
Rational neural networks are feedforward neural networks with a rational activation function. These networks find their application in approximating the solutions of PDEs, as they are able to learn the poles of meromorphic functions. In this talk we will consider the simplest rational activation function, sigma = 1/x. We will study the expressivity of such architectures (what functions we can learn) through the geometry of their neurovariety, i.e., the algebraic variety given as the Zariski closure of all possible architectures.

<span class="header-color">Speaker:</span>
Kristen Dawson (San Francisco State Univeristy)\\
<span class="header-color">Title:</span>
Positive Semidefinite Matrix Factorizations\\
<span class="header-color">Abstract:</span>
A positive semidefinite (psd) factorization of a nonnegative matrix M expresses each entry of M as the inner product of two psd matrices. These factorizations correspond to spectrahedral lifts of a polytope associated with M. The aim of the talk is to characterize the uniqueness of a psd factorization of a matrix of rank 3 using psd matrices of size 2. The characterization is obtained using tools from rigidity theory.


<span class="header-color">Speaker:</span>
[Dmitrity Morozov](https://www.mrzv.org){:target="_blank"} (Lawrence Berkeley National Laboratory) \\
<span class="header-color">Title:</span>
Recent Advances in Topological Data Analysis\\
<span class="header-color">Abstract:</span>
As topological data analysis matured over the last two decades, its applications
have evolved. And so have their needs. In this talk, we will review both
classical and recent developments in TDA, together with example applications, as
well as directions for future developments.

